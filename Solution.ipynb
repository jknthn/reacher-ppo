{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reacher - PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from unityagents import UnityEnvironment\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from model import PPOPolicyNetwork\n",
    "from agent import PPOAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Unity environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Reacher\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'environment': {\n",
    "        'state_size':  env_info.vector_observations.shape[1],\n",
    "        'action_size': brain.vector_action_space_size,\n",
    "        'number_of_agents': len(env_info.agents)\n",
    "    },\n",
    "    'pytorch': {\n",
    "        'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'discount_rate': 0.99,\n",
    "        'tau': 0.95,\n",
    "        'gradient_clip': 5,\n",
    "        'rollout_length': 2048,\n",
    "        'optimization_epochs': 10,\n",
    "        'ppo_clip': 0.2,\n",
    "        'log_interval': 2048,\n",
    "        'max_steps': 1e5,\n",
    "        'mini_batch_number': 32,\n",
    "        'entropy_coefficent': 0.01,\n",
    "        'episode_count': 250,\n",
    "        'hidden_size': 512,\n",
    "        'adam_learning_rate': 3e-4,\n",
    "        'adam_epsilon': 1e-5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_round(env, brain_name, policy, config):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]    \n",
    "    states = env_info.vector_observations                 \n",
    "    scores = np.zeros(config['environment']['number_of_agents'])                         \n",
    "    while True:\n",
    "        actions, _, _, _ = policy(states)\n",
    "        env_info = env.step(actions.cpu().detach().numpy())[brain_name]\n",
    "        next_states = env_info.vector_observations         \n",
    "        rewards = env_info.rewards                         \n",
    "        dones = env_info.local_done                     \n",
    "        scores += env_info.rewards                      \n",
    "        states = next_states                               \n",
    "        if np.any(dones):                                  \n",
    "            break\n",
    "    \n",
    "    return np.mean(scores)\n",
    "    \n",
    "def ppo(env, brain_name, policy, config, train):\n",
    "    if train:\n",
    "        optimizier = optim.Adam(policy.parameters(), config['hyperparameters']['adam_learning_rate'], \n",
    "                        eps=config['hyperparameters']['adam_epsilon'])\n",
    "        agent = PPOAgent(env, brain_name, policy, optimizier, config)\n",
    "        all_scores = []\n",
    "        averages = []\n",
    "        last_max = 30.0\n",
    "        \n",
    "        for i in tqdm.tqdm(range(config['hyperparameters']['episode_count'])):\n",
    "            agent.step()\n",
    "            last_mean_reward = play_round(env, brain_name, policy, config)\n",
    "            last_average = np.mean(np.array(all_scores[-100:])) if len(all_scores) > 100 else np.mean(np.array(all_scores))\n",
    "            all_scores.append(last_mean_reward)\n",
    "            averages.append(last_average)\n",
    "            if last_average > last_max:\n",
    "                torch.save(policy.state_dict(), f\"models/ppo-max-hiddensize-{config['hyperparameters']['hidden_size']}.pth\")\n",
    "                last_max = last_average\n",
    "            clear_output(True)\n",
    "            print('Episode: {} Total score this episode: {} Last {} average: {}'.format(i + 1, last_mean_reward, min(i + 1, 100), last_average))\n",
    "        return all_scores, averages\n",
    "    else:\n",
    "        score = play_round(env, brain_name, policy, config)\n",
    "        return [score], [score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 250/250 [1:32:09<00:00, 15.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 250 Total score this episode: 34.51249922858551 Last 100 average: 30.996649307170888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_policy = PPOPolicyNetwork(config)\n",
    "all_scores, average_scores = ppo(env, brain_name, new_policy, config, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PPOPolicyNetwork(config)\n",
    "policy.load_state_dict(torch.load('models/ppo-max-hiddensize-512.pth'))\n",
    "_, _ = ppo(env, brain_name, policy, config, train=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
